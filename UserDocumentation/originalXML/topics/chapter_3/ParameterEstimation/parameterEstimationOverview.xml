<?xml version="1.0" encoding="UTF-8"?>
<vcelldoc>
<page title="How to Parameter Estimation Overview">

	<introduction>
	<para>Parameter estimation in the Virtual Cell provides tools to optimize parameters in non-spatial deterministic models to
best fit experimental data. Parameter Estimation is run within the Virtual Cell on your own desktop
rather than on our central database servers. Once you have the results of a parameter estimation, you
will save the model parameters with the optimized parameter set as a newly created simulation. Thus,
the results will be saved permanently as part of this Virtual Cell model. 
	</para>
	<para>Current Virtual Cell has two solvers one is CFSQP(C code for Feasible Sequential Quadratic Programming) and another is Powell method. Please
	see the brief solver description below.
	</para>
	<list>
	<item>
	CFSQP is a set of C functions for the minimization of the maximum of a set of smooth objective functions (possibly a single one, or even none at all)
	subject to general smooth constraints (if there is no objective function, the goal is to simply nd a point satisfying the constraints). If the initial
	guess provided by the user is infeasible for some inequality constraint or some linear equality constraint, CFSQP rst generates a feasible point for
	these constraints; subsequently the successive iterates generated by CFSQP all satisfy these constraints. Nonlinear equality constraints are turned
	into inequality constraints (to be satised by all iterates) and the maximum of the objective functions is replaced by an exact penalty function which
	penalizes nonlinear equality constraint violations only. When solving problems with many sequentially related constraints (or objectives), such as
	discretized semiin nite programming (SIP) problems, CFSQP gives the user the option to use an algorithm that eciently solves these problems, greatly
	reducing computational eort. The user has the option of either requiring that the objective function (penalty function if nonlinear equality constraints
	are present) decrease at each iteration after feasibility for nonlinear inequality and linear constraints has been reached (monotone line search), or requiring
	a decrease within at most four iterations (nonmonotone line search). He/She must provide functions that dene the objective functions and constraint functions
	and may either provide functions to compute the respective gradients or require that CFSQP estimate them by forward nite dierences. CFSQP is an implementation
	of two algorithms based on Sequential Quadratic Programming (SQP), modied so as to generate feasible iterates. In the rst one (monotone line search), a certain
	Armijo type arc search is used with the property that the step of one is eventually accepted, a requirement for superlinear convergence. In the second one the
	same eect is achieved by means of a nonmonotone" search along a straight line. The merit function used in both searches is the maximum of the objective functions
	if there is no nonlinear equality constraints, or an exact penalty function if nonlinear equality constraints are present.
	</item>
	<para>
	For more detailed information please check out <link target = "http://drum.lib.umd.edu/bitstream/1903/5496/1/TR_94-16.pdf">CFSQP User's Guide Online</link>.
	</para>
	<item>
	Powell's method, strictly Powell's conjugate gradient descent method, is an algorithm proposed by Michael J. D. Powell for finding a local minimum of a function.
	The function need not be differentiable, and no derivatives are taken. The function must be a real-valued function of a fixed number of real-valued inputs,
	creating an N-dimensional hypersurface or Hamiltonian. The caller passes in the initial point. The caller also passes in a set of initial search vectors. 
	Typically N search vectors are passed in which are simply the normals aligned to each axis. The method minimises the function by a bi-directional search along
	each search vector, in turn. The new position can then be expressed as a linear combination of the search vectors. The new displacement vector becomes a new search
	vector, and is added to the end of the search vector list. Meanwhile the search vector which contributed most to the new direction, i.e. the one which was most
	successful, is deleted from the search vector list. The algorithm iterates an arbitrary number of times until no significant improvement is made. The method is
	useful for calculating the local minimum of a continuous but complex function, especially one without an underlying mathematical definition, because it is not
	necessary to take derivatives. The basic algorithm is simple, the complexity is in the linear searches along the search vectors.
	</item>
	</list>
		
	</introduction>
	
</page>

</vcelldoc>